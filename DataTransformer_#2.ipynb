{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validation\n",
    "#### File format validation and check for compulsory columns and total number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Unwell:  22792751.csv  has an error:  'utf-8' codec can't decode byte 0xa0 in position 20: invalid start byte\n",
      "Data transformation and validation complete.\n",
      "Skipped Files:  16\n",
      "Exception Files:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishwesh\\AppData\\Local\\Temp\\ipykernel_10972\\1172296935.py:101: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)  # 'coerce' handles invalid dates\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "# Define the input and output directories\n",
    "input_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\csv_data_2023'\n",
    "output_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\Validated_data_2023'\n",
    "\n",
    "# Remove the entire output directory\n",
    "shutil.rmtree(output_directory, ignore_errors=True)\n",
    "\n",
    "# Recreate the empty output directory\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List the CSV files in the input directory\n",
    "csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Define the desired columns (Date, Time, Direction, Vehicle Type, Occupancy)\n",
    "desired_columns = ['date', 'time', 'direction', 'type', 'occupancy']\n",
    "\n",
    "\n",
    "# Initializing list of skipped files\n",
    "skipped_files = []\n",
    "\n",
    "# Initializing list of files with exception\n",
    "files_with_exception = []\n",
    "\n",
    "\n",
    "# Dictionary with unique values in type column\n",
    "type_value_counts = {}\n",
    "\n",
    "\n",
    "\n",
    "# Standardizing vehicle types\n",
    "def standardize_vehicle_types(vehicle_type):\n",
    "    if vehicle_type in ('car','bus','bicycle','taxi','van','motorbike','lorry','scooter'):\n",
    "        return vehicle_type\n",
    "    elif vehicle_type in ('mini truck', 'truck'):\n",
    "        return 'lorry'\n",
    "    elif vehicle_type in ('bike','motorcycle'):\n",
    "        return 'motorbike'\n",
    "    else:\n",
    "        return 'other' # Others/Unknown values \n",
    "\n",
    "\n",
    "# Standardizing Occupancy\n",
    "def standardize_occupancy(occupancy_value):\n",
    "    try:\n",
    "        return float(occupancy_value)\n",
    "    except ValueError:\n",
    "        percentage=float(occupancy_value.strip().rstrip('%'))\n",
    "        return round((percentage/100)*(95))\n",
    "\n",
    "# Iterate through each CSV file in the input directory\n",
    "for file in csv_files:\n",
    "\n",
    "    try:    \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(input_directory, file),encoding='utf-8')\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "        \n",
    "        # Validate that the required columns exist in the DataFrame\n",
    "        all_columns_present = True\n",
    "\n",
    "        #print(df.columns)\n",
    "        for col in desired_columns:\n",
    "            if col.strip().lower() not in df.columns:\n",
    "                all_columns_present = False\n",
    "                break\n",
    "        \n",
    "        # Check if all desired columns exist in the DataFrame\n",
    "        if (not all_columns_present) or (len(df.columns)!=7):\n",
    "            #print(f\"Skipping {file} due to missing columns\")\n",
    "            skipped_files.append(file)\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        for col in df.columns:\n",
    "            \n",
    "            # Convert 'Date' to a specific format\n",
    "            \n",
    "            if col.strip().lower() == 'date':\n",
    "                transformation = False\n",
    "                #print(\"date before: \", df[col][0])\n",
    "                if transformation == False:\n",
    "                    try:\n",
    "                        df['Parsed_Date'] = df[col].apply(lambda x: parse(x, fuzzy=True, dayfirst=True))\n",
    "                        #print(\" Parsed date: \", df['Parsed_Date'][0], flush=True)\n",
    "                        df[col] = df['Parsed_Date'].dt.strftime('%d-%m-%Y')\n",
    "                        df.drop('Parsed_Date', axis=1, inplace=True)\n",
    "                        transformation =  True\n",
    "                        #print(transformation)\n",
    "                    except Exception as x:\n",
    "                        pass\n",
    "                        #print(transformation)\n",
    "                if transformation == False:\n",
    "                    try:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)  # 'coerce' handles invalid dates\n",
    "                        df[col] = df[col].dt.strftime('%d-%m-%Y')\n",
    "                        transformation =  True\n",
    "                        #print(transformation)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                #print(\"date after: \", df[col][0])\n",
    "                \n",
    "            # Covert 'Time' to a specific format\n",
    "            \n",
    "            if col.strip().lower() == 'time':\n",
    "                #print(\"Time before: \",df[col][0])\n",
    "                df[col] = df[col].str.replace(r'am', '', case=False).str.strip()\n",
    "                #print(\"Time without am/pm\", df[col][0])\n",
    "                x=df[col][0]\n",
    "\n",
    "                time_format_boolean = False\n",
    "                \n",
    "                \n",
    "                if df[col].str.contains('.', regex=False).any():\n",
    "                    df[col] = pd.to_datetime(df[col], format='%I.%M', errors='coerce')\n",
    "                    df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "                    time_format_boolean = True\n",
    "                    #print(\"Time after: \",df[col][0])\n",
    "                \n",
    "                if (df[col].str.contains(':', regex=False).any()) and (not time_format_boolean):\n",
    "                    colon_count = df[col].str.count(':').max()\n",
    "                    \n",
    "                    if colon_count == 1:\n",
    "                        time_format = '%H:%M'  # Format for '10:35am' or '10:35pm'\n",
    "                    else:\n",
    "                        time_format = '%H:%M:%S' \n",
    "                    \n",
    "                    df[col] = pd.to_datetime(df[col],format=time_format, errors='coerce')\n",
    "                    df[col] = df[col].dt.strftime('%H:%M:%S')\n",
    "                    #print(\"Time after: \",df[col][0])               \n",
    "            \n",
    "            # Convert 'Direction' to a specific format\n",
    "            if col.strip().lower() == 'direction':\n",
    "                #print(\"Before:\",df[col][0])\n",
    "                df[col] = df[col].str.lower().str.strip()\n",
    "                #print(\"After:\",df[col][0])\n",
    "            \n",
    "            # Convert 'Type' to a specific format\n",
    "            if col.strip().lower() == 'type':\n",
    "                df[col] = df[col].str.lower().str.strip()\n",
    "                df[col] = df[col].apply(standardize_vehicle_types)\n",
    "\n",
    "            # Convert 'Occupancy' to a specific format\n",
    "            if col.strip().lower() == 'occupancy':\n",
    "                df[col] = df[col].apply(standardize_occupancy)\n",
    "\n",
    "                        \n",
    "            # You can add similar transformations for other desired columns here\n",
    "\n",
    "        # Specify the output file path\n",
    "        output_file = os.path.join(output_directory, f'transformed_{file}')\n",
    "        \n",
    "        # Save the DataFrame with transformed and additional columns\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        #print(f\"Transformed and saved {file} to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"File Unwell: \", file,\" has an error: \",e)\n",
    "        files_with_exception.append(file)\n",
    "\n",
    "#for key, value in type_value_counts.items():\n",
    "#    print(str(key)+\":\"+str(value))    \n",
    "\n",
    "print(\"Data transformation and validation complete.\")\n",
    "print(\"Skipped Files: \", len(skipped_files))\n",
    "print(\"Exception Files: \", len(files_with_exception))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Additional Columns\n",
    "#### Checking and grouping columns included in additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle age(years): 2 files - ['transformed_22792701.csv', 'transformed_22792711.csv']\n",
      "brand name: 2 files - ['transformed_22792701.csv', 'transformed_22792711.csv']\n",
      "additionaldata1: 1 files - ['transformed_22792702.csv']\n",
      "additionaldata2: 7 files - ['transformed_22792702.csv', 'transformed_22792721.csv', 'transformed_22792742.csv', 'transformed_22792767.csv', 'transformed_22792780.csv', 'transformed_22792788.csv', 'transformed_22792795.csv']\n",
      "color: 2 files - ['transformed_22792703.csv', 'transformed_22792803.csv']\n",
      "public/private: 1 files - ['transformed_22792703.csv']\n",
      "colour: 10 files - ['transformed_22792704.csv', 'transformed_22792708.csv', 'transformed_22792725.csv', 'transformed_22792732.csv', 'transformed_22792747.csv', 'transformed_22792754.csv', 'transformed_22792762.csv', 'transformed_22792785.csv', 'transformed_22792805.csv', 'transformed_22792806.csv']\n",
      "turn signal: 2 files - ['transformed_22792704.csv', 'transformed_22792785.csv']\n",
      "carpool: 1 files - ['transformed_22792706.csv']\n",
      "rideshare: 1 files - ['transformed_22792706.csv']\n",
      "vehicle category: 1 files - ['transformed_22792708.csv']\n",
      "driver gender: 4 files - ['transformed_22792714.csv', 'transformed_22792729.csv', 'transformed_22792768.csv', 'transformed_22792799.csv']\n",
      "fuel type: 4 files - ['transformed_22792714.csv', 'transformed_22792742.csv', 'transformed_22792788.csv', 'transformed_22792799.csv']\n",
      "is_carpool: 1 files - ['transformed_22792717.csv']\n",
      "is_rideshare: 1 files - ['transformed_22792717.csv']\n",
      "ev: 2 files - ['transformed_22792719.csv', 'transformed_22792765.csv']\n",
      "car brand: 2 files - ['transformed_22792719.csv', 'transformed_22792765.csv']\n",
      "electric: 4 files - ['transformed_22792720.csv', 'transformed_22792749.csv', 'transformed_22792755.csv', 'transformed_22792779.csv']\n",
      "part of mobility service: 1 files - ['transformed_22792720.csv']\n",
      "origin: 2 files - ['transformed_22792721.csv', 'transformed_22792795.csv']\n",
      "age_id: 2 files - ['transformed_22792722.csv', 'transformed_22792784.csv']\n",
      "is_env_friendly: 1 files - ['transformed_22792722.csv']\n",
      "brand: 9 files - ['transformed_22792725.csv', 'transformed_22792732.csv', 'transformed_22792734.csv', 'transformed_22792736.csv', 'transformed_22792737.csv', 'transformed_22792747.csv', 'transformed_22792762.csv', 'transformed_22792764.csv', 'transformed_22792798.csv']\n",
      "service: 2 files - ['transformed_22792728.csv', 'transformed_22792790.csv']\n",
      "size: 2 files - ['transformed_22792728.csv', 'transformed_22792790.csv']\n",
      "vehicle purpose: 2 files - ['transformed_22792729.csv', 'transformed_22792768.csv']\n",
      "driver_gender: 2 files - ['transformed_22792730.csv', 'transformed_22792771.csv']\n",
      "mode_of_travel: 2 files - ['transformed_22792730.csv', 'transformed_22792771.csv']\n",
      "public vehicle: 2 files - ['transformed_22792733.csv', 'transformed_22792746.csv']\n",
      "safety: 2 files - ['transformed_22792733.csv', 'transformed_22792746.csv']\n",
      "age identifier: 2 files - ['transformed_22792734.csv', 'transformed_22792754.csv']\n",
      "year: 1 files - ['transformed_22792736.csv']\n",
      "age group: 2 files - ['transformed_22792737.csv', 'transformed_22792798.csv']\n",
      "family car: 2 files - ['transformed_22792749.csv', 'transformed_22792779.csv']\n",
      "zero emissions: 1 files - ['transformed_22792752.csv']\n",
      "low emissions compliant: 1 files - ['transformed_22792752.csv']\n",
      "mobilityservice: 1 files - ['transformed_22792755.csv']\n",
      "vehicle brand: 1 files - ['transformed_22792759.csv']\n",
      "vehicle year: 1 files - ['transformed_22792759.csv']\n",
      "weather: 2 files - ['transformed_22792763.csv', 'transformed_22792800.csv']\n",
      "area_code: 2 files - ['transformed_22792763.csv', 'transformed_22792800.csv']\n",
      "ageidentifier: 2 files - ['transformed_22792764.csv', 'transformed_22792806.csv']\n",
      "eco-friendly: 1 files - ['transformed_22792767.csv']\n",
      "green vehicle: 2 files - ['transformed_22792772.csv', 'transformed_22792802.csv']\n",
      "date code: 2 files - ['transformed_22792772.csv', 'transformed_22792802.csv']\n",
      "tint > 50%?: 1 files - ['transformed_22792775.csv']\n",
      "high tinted area: 1 files - ['transformed_22792775.csv']\n",
      "hasgreennumberplate: 1 files - ['transformed_22792780.csv']\n",
      "tint area: 1 files - ['transformed_22792781.csv']\n",
      "parsed_date: 1 files - ['transformed_22792781.csv']\n",
      "is_env_fr: 1 files - ['transformed_22792784.csv']\n",
      "make: 1 files - ['transformed_22792786.csv']\n",
      "zero emission: 1 files - ['transformed_22792786.csv']\n",
      "ecofriendly: 1 files - ['transformed_22792794.csv']\n",
      "date of vehicle: 1 files - ['transformed_22792794.csv']\n",
      "window_tint: 1 files - ['transformed_22792796.csv']\n",
      "rim_type: 1 files - ['transformed_22792796.csv']\n",
      "public / private: 1 files - ['transformed_22792803.csv']\n",
      "purpose: 1 files - ['transformed_22792805.csv']\n"
     ]
    }
   ],
   "source": [
    "input_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\Validated_data_2023'\n",
    "\n",
    "# Initialize a dictionary to store column occurrences\n",
    "column_occurrences = {}\n",
    "\n",
    "# List the CSV files in the input directory\n",
    "csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file in the input directory\n",
    "for file in csv_files:\n",
    "\n",
    "    try:    \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(input_directory, file),encoding='utf-8')\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # Extract the last two columns\n",
    "        last_two_columns = df.iloc[:, -2:].columns.tolist()\n",
    "\n",
    "        # Update the dictionary with column occurrences\n",
    "        for column in last_two_columns:\n",
    "            if column in column_occurrences:\n",
    "                column_occurrences[column].append(file)\n",
    "            else:\n",
    "                column_occurrences[column] = [file]\n",
    "\n",
    "                \n",
    "    except Exception as e:\n",
    "        print('File: ', file, 'has error: ', e)\n",
    "\n",
    "# Print the dictionary of column occurrences\n",
    "for column, files in column_occurrences.items():\n",
    "    print(f\"{column}: {len(files)} files - {files}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Column Data Transformation and Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied and column names standardized.\n"
     ]
    }
   ],
   "source": [
    "input_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\Validated_data_2023'\n",
    "output_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3'\n",
    "\n",
    "# Encoding into 1s and 0s\n",
    "def standardize_electric_column(x):\n",
    "    x['electric'] = x['electric'].astype(str).str.strip().str.lower().replace({'yes': 1, 'ev': 1, '1': 1, 'no': 0, 'non ev': 0, '0': 0})\n",
    "    return x\n",
    "\n",
    "# Create a mapping dictionary for column names\n",
    "column_mapping = {'electric': ['electric', 'ev']}\n",
    "\n",
    "# Remove the entire output directory\n",
    "for standard_name, variations in column_mapping.items():\n",
    "    directory = os.path.join(output_root_directory, standard_name)\n",
    "    shutil.rmtree(directory, ignore_errors=True)\n",
    "\n",
    "# List the CSV files in the input directory\n",
    "csv_files = [f for f in os.listdir(input_root_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file in the input directory\n",
    "for file in csv_files:\n",
    "    try:    \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(input_root_directory, file),encoding='utf-8')\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "        \n",
    "        # Iterate through the mapping dictionary\n",
    "        for standard_name, variations in column_mapping.items():\n",
    "            matching_columns = []\n",
    "            for variation in variations:\n",
    "                if variation.lower() in df.columns:\n",
    "                    matching_columns.append(variation)\n",
    "\n",
    "            if matching_columns:\n",
    "                # Rename the matching columns to the standard name\n",
    "                df.rename(columns={col: 'electric' for col in matching_columns}, inplace=True)\n",
    "\n",
    "                # Select only the desired columns\n",
    "                desired_columns = ['date', 'time', 'direction', 'type', 'occupancy', 'electric']\n",
    "                df = df[desired_columns]\n",
    "                \n",
    "                # Apply the standardization function\n",
    "                df = standardize_electric_column(df)\n",
    "                \n",
    "                # Create the output directory based on the standard name\n",
    "                output_directory = os.path.join(output_root_directory, standard_name)\n",
    "                os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "                # Copy the DataFrame to the output directory with the standardized column name\n",
    "                output_file = os.path.join(output_directory, f\"{standard_name}_{file}\")\n",
    "                df.to_csv(output_file, index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"File Unwell: \", file,\" has an error: \",e)\n",
    "\n",
    "# Print a message indicating the process is complete\n",
    "print(\"Files copied and column names standardized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Data Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file :  electric_transformed_22792719.csv\n",
      "('24-10-2023', '09:40:00') :  3\n",
      "('24-10-2023', '09:45:00') :  6\n",
      "('24-10-2023', '09:50:00') :  13\n",
      "('24-10-2023', '09:55:00') :  10\n",
      "('24-10-2023', '10:00:00') :  9\n",
      "('24-10-2023', '10:05:00') :  10\n",
      "('24-10-2023', '10:10:00') :  6\n",
      "file :  electric_transformed_22792720.csv\n",
      "('24-10-2023', '10:30:00') :  14\n",
      "('24-10-2023', '10:35:00') :  10\n",
      "('24-10-2023', '10:40:00') :  4\n",
      "('24-10-2023', '10:45:00') :  5\n",
      "('24-10-2023', '10:50:00') :  7\n",
      "('24-10-2023', '10:55:00') :  9\n",
      "file :  electric_transformed_22792749.csv\n",
      "('25-10-2023', '09:30:00') :  10\n",
      "('25-10-2023', '09:35:00') :  5\n",
      "('25-10-2023', '09:40:00') :  12\n",
      "('25-10-2023', '09:45:00') :  6\n",
      "('25-10-2023', '09:50:00') :  9\n",
      "('25-10-2023', '09:55:00') :  13\n",
      "file :  electric_transformed_22792755.csv\n",
      "('24-10-2023', '10:30:00') :  32\n",
      "('24-10-2023', '10:35:00') :  16\n",
      "('24-10-2023', '10:40:00') :  19\n",
      "('24-10-2023', '10:45:00') :  29\n",
      "('24-10-2023', '10:50:00') :  26\n",
      "('24-10-2023', '10:55:00') :  31\n",
      "file :  electric_transformed_22792765.csv\n",
      "('24-10-2023', '09:40:00') :  10\n",
      "('24-10-2023', '09:45:00') :  18\n",
      "('24-10-2023', '09:50:00') :  22\n",
      "('24-10-2023', '09:55:00') :  30\n",
      "('24-10-2023', '10:00:00') :  23\n",
      "('24-10-2023', '10:05:00') :  43\n",
      "('24-10-2023', '10:10:00') :  17\n",
      "file :  electric_transformed_22792779.csv\n",
      "('25-10-2023', '09:30:00') :  28\n",
      "('25-10-2023', '09:35:00') :  27\n",
      "('25-10-2023', '09:40:00') :  29\n",
      "('25-10-2023', '09:45:00') :  25\n",
      "('25-10-2023', '09:50:00') :  32\n",
      "('25-10-2023', '09:55:00') :  35\n"
     ]
    }
   ],
   "source": [
    "input_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\electric'\n",
    "output_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3'\n",
    "\n",
    "\n",
    "# List the CSV files in the input directory\n",
    "csv_files = [f for f in os.listdir(input_root_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Dictionary to store the number of records for each date and time\n",
    "records_count_list = []\n",
    "\n",
    "# Iterate through each CSV file in the input directory\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(input_root_directory, file), encoding='utf-8')\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "        # Group by date and time and count the number of records\n",
    "        grouped_df = df.groupby(['date', 'time']).size().reset_index(name='records_count')\n",
    "\n",
    "        # Dictionary to store records for the current file\n",
    "        file_dict = {'file': file}\n",
    "\n",
    "        # Update the file_dict with the maximum records for each date and time\n",
    "        for index, row in grouped_df.iterrows():\n",
    "            key = (row['date'], row['time'])\n",
    "            file_dict[key] = row['records_count']\n",
    "\n",
    "        # Append the file_dict to the records_count_list\n",
    "        records_count_list.append(file_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"File Unwell: \", file, \" has an error: \", e)\n",
    "\n",
    "\n",
    "            \n",
    "# Print the dictionary\n",
    "for i in records_count_list:\n",
    "    for key, value in i.items():\n",
    "        print(key,': ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('24-10-2023', '09:40:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 10}\n",
      "('24-10-2023', '09:45:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 18}\n",
      "('24-10-2023', '09:50:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 22}\n",
      "('24-10-2023', '09:55:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 30}\n",
      "('24-10-2023', '10:00:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 23}\n",
      "('24-10-2023', '10:05:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 43}\n",
      "('24-10-2023', '10:10:00') :  {'file': 'electric_transformed_22792765.csv', 'records': 17}\n",
      "('24-10-2023', '10:30:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 32}\n",
      "('24-10-2023', '10:35:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 16}\n",
      "('24-10-2023', '10:40:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 19}\n",
      "('24-10-2023', '10:45:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 29}\n",
      "('24-10-2023', '10:50:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 26}\n",
      "('24-10-2023', '10:55:00') :  {'file': 'electric_transformed_22792755.csv', 'records': 31}\n",
      "('25-10-2023', '09:30:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 28}\n",
      "('25-10-2023', '09:35:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 27}\n",
      "('25-10-2023', '09:40:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 29}\n",
      "('25-10-2023', '09:45:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 25}\n",
      "('25-10-2023', '09:50:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 32}\n",
      "('25-10-2023', '09:55:00') :  {'file': 'electric_transformed_22792779.csv', 'records': 35}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the file with maximum records for each date-time pair\n",
    "max_records_files = {}\n",
    "\n",
    "# Iterate through the list of dictionaries\n",
    "for data_dict in records_count_list:\n",
    "    # Iterate through the keys of each dictionary\n",
    "    for key in data_dict.keys():\n",
    "        # Check if the key is a tuple (date-time pair)\n",
    "        if isinstance(key, tuple):\n",
    "            # Check if this file has more records than the current maximum for the date-time pair\n",
    "            if key not in max_records_files or data_dict[key] > max_records_files[key]['records']:\n",
    "                max_records_files[key] = {'file': data_dict['file'], 'records': data_dict[key]}\n",
    "\n",
    "\n",
    "# Print the unique date-time pairs\n",
    "for key,value in max_records_files.items():\n",
    "    print(key, ': ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated records saved to D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\electric\\electric_consolidated\\electric_consolidated.csv\n"
     ]
    }
   ],
   "source": [
    "input_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3\\electric'\n",
    "output_root_directory = r'D:\\University of Bath\\Assignments\\Applied Data Science\\Task 3'\n",
    "\n",
    "# Remove the entire output directory\n",
    "directory = os.path.join(input_root_directory, 'electric_consolidated')\n",
    "shutil.rmtree(directory, ignore_errors=True)\n",
    "\n",
    "# Create an empty DataFrame to store consolidated records\n",
    "consolidated_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through the keys of the max_records_files dictionary\n",
    "for date_time_pair, file_info in max_records_files.items():\n",
    "    # Extract the file with maximum records for the date-time pair\n",
    "    file_path = os.path.join(input_root_directory, file_info['file'])\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract records for the specific date-time pair\n",
    "    date, time = date_time_pair\n",
    "    records_for_date_time_pair = df[(df['date'] == date) & (df['time'] == time)]\n",
    "    \n",
    "    # Append the extracted records to the consolidated DataFrame\n",
    "    consolidated_df = pd.concat([consolidated_df, records_for_date_time_pair], ignore_index=True)\n",
    "\n",
    "# Specify the output directory for the consolidated records\n",
    "output_directory = os.path.join(output_root_directory, 'electric', 'electric_consolidated')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Specify the output file path for the consolidated records\n",
    "output_file_path = os.path.join(output_directory, 'electric_consolidated.csv')\n",
    "\n",
    "# Save the consolidated DataFrame to a CSV file\n",
    "consolidated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Consolidated records saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
